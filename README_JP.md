# 最高情報責任者（Wiseflow）

**[English](README_EN.md) | [中文](README.md) | [한국어](README_KR.md)**

🚀 **最高情報責任者**（Wiseflow）は、ウェブサイト、WeChat公式アカウント、ソーシャルプラットフォームなど、さまざまな情報源から設定された焦点に基づいて情報を抽出し、自動的にラベル付けしてデータベースにアップロードするアジャイルな情報マイニングツールです。

**私たちが欠けているのは情報ではなく、大量の情報からノイズをフィルタリングして価値ある情報を明らかにする能力です。**

🌱 最高情報責任者がどのようにあなたの時間を節約し、無関係な情報をフィルタリングし、注目すべきポイントを整理するかを見てみましょう！ 🌱

- ✅ 汎用ウェブコンテンツパーサー、統計学習（オープンソースプロジェクトGNEに依存）とLLMを包括的に使用し、90%以上のニュースページに適合;
  （**WiseflowはWeChat公式アカウントの記事から情報を抽出することに特に優れており**、専用のmp記事パーサーを設定しています！）
- ✅ 非同期タスクアーキテクチャ;
- ✅ LLMを使用した情報抽出とラベル分類（9BサイズのLLMで完璧にタスクを実行できます）！

https://github.com/TeamWiseFlow/wiseflow/assets/96130569/bd4b2091-c02d-4457-9ec6-c072d8ddfb16

<img alt="sample.png" src="asset/sample.png" width="1024"/>

## 🔥 V0.3.1 アップデート

👏 9BサイズのLLM（THUDM/glm-4-9b-chat）の一部は、安定した情報抽出出力を実現できますが、複雑な意味のタグ（「党建設」など）や特定の収集が必要なタグ（「コミュニティ活動」のみを収集し、コンサートなどの大規模なイベント情報は含まない）については、
現在のプロンプトでは正確な抽出ができません。そこで、このバージョンでは各タグに説明フィールドを追加し、入力によってより明確なタグ指定ができるようにしました。

   _注：複雑な説明にはより大規模なモデルが必要です。詳細は [モデル推奨 2024-09-03](###-4. モデル推奨 [2024-09-03]) を参照してください_

👏 また、前バージョンのプロンプト言語選択の問題（出力結果には影響しません）に対処し、現在のバージョンではさらにシンプルなソリューションを採用しています。ユーザーはシステム言語を指定する必要がなくなりました（Dockerではそれほど直感的ではありません）、システムはタグとその説明に基づいてプロンプトの言語（つまり情報の出力言語）を判断し、wiseflowの展開と使用をさらに簡素化します。**ただし、現在wiseflowは簡体字中国語と英語のみをサポートしており、他の言語のニーズはcore/insights/get_info.pyのプロンプトを変更することで実現できます**

🌹 また、このアップデートでは過去2か月間のPRをマージし、以下の新しい貢献者が追加されました：

@wwz223 @madizm @GuanYixuan @xnp2020 @JimmyMa99

🌹 皆さんの貢献に感謝します！

## 🌟 アプリケーションにwiseflowを統合する方法

wiseflowはネイティブLLMアプリケーションで、7B〜9BサイズのLLMで情報マイニング、フィルタリング、分類タスクをうまく実行でき、ベクトルモデルを必要とせず、さまざまなハードウェア環境でのローカルおよびプライベート展開に適しています。

wiseflowはマイニングされた情報を組み込みのPocketbaseデータベースに保存します。つまり、wiseflowのコードを深く理解する必要はなく、データベースの読み取り操作だけで統合が可能です！

PocketBaseは人気のある軽量データベースで、現在Go/Javascript/Pythonなどの言語のSDKがあります。
   - Go : https://pocketbase.io/docs/go-overview/
   - Javascript : https://pocketbase.io/docs/js-overview/
   - python : https://github.com/vaphes/pocketbase

## 🔄 wiseflowと一般的なクローラーツール、LLM-Agentプロジェクトの違いと関連性

| 特徴 | 最高情報責任者（Wiseflow） | クローラー / スクレイパー | LLM-Agent |
|----------------|--------------------------------------|-------------------|-----------|
| **主な解決課題** | データ処理（フィルタリング、精製、ラベリング） | 生データ取得 | 下流アプリケーション |
| **関連性** | | WiseFlowに統合可能、wiseflowにより強力な生データ取得能力を与える | WiseFlowを動的知識ベースとして統合可能 |

## 📥 インストールと使用方法

### 1. リポジトリのクローン

🌹 スターリングとフォークは良い習慣です 🌹

```bash
git clone https://github.com/TeamWiseFlow/wiseflow.git
cd wiseflow
```

### 2. Dockerを使用して実行することを推奨

```bash
docker compose up
```

注意：

- wiseflowコードリポジトリのルートディレクトリで上記のコマンドを実行してください;

- 実行前に.envファイルを作成し、Dockerfileと同じディレクトリ（wiseflowコードリポジトリのルートディレクトリ）に配置してください。.envファイルはenv_sampleを参照できます;

- 初回のdockerコンテナ実行時にエラーが発生することがありますが、これはpbリポジトリの管理者アカウントがまだ作成されていないためです。

- この時点でコンテナを実行したままにし、ブラウザを開いてhttp://127.0.0.1:8090/_/にアクセスし、プロンプトに従って管理者アカウントを作成してください（メールアドレスを使用する必要があります）。そして、作成した管理者のメールアドレス（再び、メールアドレスを使用する必要があります）とパスワードを.envファイルに入力し、コンテナを再起動してください。

_コンテナのタイムゾーンと言語を変更したい場合は、以下のコマンドでイメージを実行してください_

```bash
docker run -e LANG=zh_CN.UTF-8 -e LC_CTYPE=zh_CN.UTF-8 your_image
```

### 2. [代替案] Pythonを直接使用して実行

```bash
conda create -n wiseflow python=3.10
conda activate wiseflow
cd core
pip install -r requirements.txt
```

その後、core/scriptsのスクリプトを参照してpb、task、backendを個別に起動してください（スクリプトファイルをcoreディレクトリに移動してください）

注意：

- pbを最初に起動する必要があります。taskとbackendは独立したプロセスで、起動順序は問いません。必要に応じてどちらか一方だけを起動することもできます;

- https://pocketbase.io/docs/からデバイスに合わせたpocketbaseクライアントをダウンロードし、/core/pbディレクトリに配置してください;

- pbの実行時の問題（初回実行時のエラーを含む）については、core/pb/README.mdを参照してください;

- 使用前に.envファイルを作成し、wiseflowコードリポジトリのルートディレクトリ（coreディレクトリの親ディレクトリ）に配置してください。.envファイルはenv_sampleを参照できます。詳細な設定説明は以下を参照してください;

📚 開発者向け、詳細は/core/README.mdを参照してください

pocketbaseを介して取得したデータ：

http://127.0.0.1:8090/_/ - 管理者ダッシュボードUI

http://127.0.0.1:8090/api/ - REST API

### 3. 設定

ディレクトリ内のenv_sampleをコピーし、.envに名前を変更して、以下のように設定情報（LLMサービストークンなど）を入力してください：

WindowsユーザーがPythonプログラムを直接実行する場合、「スタート - 設定 - システム - バージョン情報 - 詳細システム設定 - 環境変数」で以下の項目を設定し、ターミナルを再起動して有効にしてください

- LLM_API_KEY # 大規模モデル推論サービスAPIキー

- LLM_API_BASE # このプロジェクトはopenai sdkに依存しており、モデルサービスがopenaiインターフェースをサポートしている場合、この項目を設定することで正常に使用できます。openaiサービスを使用する場合、この項目を削除してください

- WS_LOG="verbose" # デバッグ観察を開始するかどうかを設定します。必要がない場合は削除してください

- GET_INFO_MODEL # 情報抽出とラベルマッチングタスクのモデル、デフォルトはgpt-4o-mini-2024-07-18

- REWRITE_MODEL # 類似情報のマージと書き換えタスクのモデル、デフォルトはgpt-4o-mini-2024-07-18

- HTML_PARSE_MODEL # ウェブページ解析モデル（GNEアルゴリズムが効果的でない場合に自動的に有効になります）、デフォルト
- PROJECT_DIR # データ、キャッシュおよびログファイルの保存場所、コードリポジトリに対する相対パス、デフォルトでは空欄でコードリポジトリ内に保存
- PB_API_AUTH='email|password' # pbデータベース管理者のメールアドレスとパスワード（必ずメールアドレスを使用してください、フィクションのメールアドレスでも可）
- PB_API_BASE  # 通常の使用にはこの項目は必要ありません、デフォルトのpocketbaseローカルインターフェース（8090）を使用しない場合のみ指定します

### 4. モデル推奨 [2024-09-03]

反復テストを経て（中国語・英語タスク）**GET_INFO_MODEL**、**REWRITE_MODEL**、**HTML_PARSE_MODEL** の最小利用可能なモデルはそれぞれ：**"THUDM/glm-4-9b-chat"**、**"Qwen/Qwen2-7B-Instruct"**、**"Qwen/Qwen2-7B-Instruct"** です。

現在、SiliconFlowはQwen2-7B-Instruct、glm-4-9b-chatのオンライン推論サービスが無料であることを公式に発表しました。これは、あなたが「ゼロコスト」でwiseflowを利用できることを意味します！

😄 ご好意ある限り、私の[siliconflow招待リンク](https://cloud.siliconflow.cn?referrer=clx6wrtca00045766ahvexw92)を使用していただけると、私もより多くのトークン報酬を得ることができます 😄

⚠️ **V0.3.1 更新**

説明付きの複雑なタグを使用している場合、glm-4-9b-chat規模のモデルでは正確な理解が保証できません。現時点でのテスト結果では、この種類のタスクに対して効果が高いモデルは **Qwen/Qwen2-72B-Instruct** と **gpt-4o-mini-2024-07-18** です。

### 5. **関心事項と定期スキャン情報ソースの追加**

プログラムを起動したら、pocketbase AdminダッシュボードUI (http://127.0.0.1:8090/_/) を開きます。

#### 5.1 tagsフォームを開く

このフォームを通じてあなたの関心事項を指定することができます。LLMはこれに基づいて情報を抽出、フィルタリングし、分類します。

tags のフィールド説明：
   - name, 関心事項の名前
   - explaination, 関心事項の詳細な説明または具体的な規定、例えば「上海市の公的な中学校進学情報のみ」（タグ名は「上海中学校進学情報」）
   - activated, 有効化されているかどうか。無効化するとその関心事項は無視されます。無効化後も再度有効化することができます。Dockerコンテナの再起動は必要ありません、次の定期タスク時に更新されます。

#### 5.2 sitesフォームを開く

このフォームを通じてカスタム情報ソースを指定できます。システムはバックグラウンドで定期タスクを開始し、情報ソースのスキャン、解析、分析を行います。

sites のフィールド説明：
   - url, 情報ソースのURL、具体的な記事ページではなくリストページを指定してください。
   - per_hours, スキャン頻度、時間単位、整数型（1～24の範囲、スキャン頻度は1日に1回を超えないように、つまり24と設定することを推奨します）
   - activated, 有効化されているかどうか。無効化するとその情報ソースは無視されます。無効化後も再度有効化することができます。Dockerコンテナの再起動は必要ありません、次の定期タスク時に更新されます。

### 6. ローカル展開

ご覧の通り、本プロジェクトは最低でも7b/9bサイズのLLMを使用するだけでよく、任意のベクトルモデルは必要ありません。つまり、3090RTX（24GB VRAM）があれば本プロジェクトを完全にローカルで展開することが可能です。

あなたのローカル展開LLMサービスがopenai SDKと互換性があることを確認し、LLM_API_BASE を設定してください。

注：7b～9b規模のLLMがタグの説明を正確に理解できるようにするためには、dspyを使用したプロンプトの最適化が推奨されますが、これは約50件の手動ラベル付けデータの累積が必要となります。詳しくは [DSPy](https://dspy-docs.vercel.app/) をご覧ください。

## 🛡️ ライセンス

本プロジェクトは [Apache2.0](LICENSE) ライセンスの元でオープンソースです。

商用利用およびカスタマイズ協力については、**Email：35252986@qq.com** までお問い合わせください。

   - コマーシャル顧客は私たちに報告して登録してください、製品は永久に無料です。

## 📬 連絡先

何か質問や提案がありましたら、[issue](https://github.com/TeamWiseFlow/wiseflow/issues) を通じて私たちと連絡を取りましょう。

## 🤝 本プロジェクトは以下の優れたオープンソースプロジェクトに基づいています：

- GeneralNewsExtractor （ General Extractor of News Web Page Body Based on Statistical Learning） https://github.com/GeneralNewsExtractor/GeneralNewsExtractor
- json_repair（Repair invalid JSON documents ） https://github.com/josdejong/jsonrepair/tree/main 
- python-pocketbase (pocketBase client SDK for python) https://github.com/vaphes/pocketbase

## Citation

本プロジェクトの一部または全部を参考にしたり引用したりする場合は、以下の情報を明記してください：

```
Author：Wiseflow Team
https://github.com/TeamWiseFlow/wiseflow
Licensed under Apache2.0
```

