# WiseFlow

**[‰∏≠Êñá](README_CN.md) | [Êó•Êú¨Ë™û](README_JP.md) | [Fran√ßais](README_FR.md) | [English](README.md)**

**Wiseflow** ist ein agiles Information-Mining-Tool, das in der Lage ist, pr√§gnante Nachrichten aus verschiedenen Quellen wie Webseiten, offiziellen WeChat-Konten, sozialen Plattformen usw. zu extrahieren. Es kategorisiert die Informationen automatisch mit Tags und l√§dt sie in eine Datenbank hoch.

Es mangelt uns nicht an Informationen, sondern wir m√ºssen den L√§rm herausfiltern, um wertvolle Informationen hervorzuheben! 

Sehen Siewie WiseFlow Ihnen hilft, Zeit zu sparen, irrelevante Informationen zu filtern und interessante Punkte zu organisieren!

https://github.com/TeamWiseFlow/wiseflow/assets/96130569/bd4b2091-c02d-4457-9ec6-c072d8ddfb16

<img alt="sample.png" src="asset/sample.png" width="1024"/>

## üî• Wichtige Updates in V0.3.0

- ‚úÖ Neuer universeller Web-Content-Parser, der auf GNE (ein Open-Source-Projekt) und LLM basiert und mehr als 90% der Nachrichtenseiten unterst√ºtzt.

- ‚úÖ Neue asynchrone Aufgabenarchitektur.

- ‚úÖ Neue Strategie zur Informationsextraktion und Tag-Klassifizierung, die pr√§ziser und feiner ist und Aufgaben mit nur einem 9B LLM perfekt ausf√ºhrt.

## üåü Hauptfunktionen

- üöÄ **Native LLM-Anwendung**  
  Wir haben die am besten geeigneten Open-Source-Modelle von 7B~9B sorgf√§ltig ausgew√§hlt, um die Nutzungskosten zu minimieren und es datensensiblen Benutzern zu erm√∂glichen, jederzeit vollst√§ndig auf eine lokale Bereitstellung umzuschalten.


- üå± **Leichtes Design**  
  Ohne Vektormodelle ist das System minimal invasiv und ben√∂tigt keine GPUs, was es f√ºr jede Hardwareumgebung geeignet macht.


- üóÉÔ∏è **Intelligente Informationsextraktion und -klassifizierung**  
  Extrahiert automatisch Informationen aus verschiedenen Quellen und markiert und klassifiziert sie basierend auf den Interessen der Benutzer.

  üòÑ **Wiseflow ist besonders gut darin, Informationen aus WeChat-Official-Account-Artikeln zu extrahieren**; hierf√ºr haben wir einen dedizierten Parser f√ºr mp-Artikel eingerichtet!


- üåç **Kann in jedes RAG-Projekt integriert werden**  
  Kann als dynamische Wissensdatenbank f√ºr jedes RAG-Projekt dienen, ohne dass der Code von Wiseflow verstanden werden muss. Es reicht, die Datenbank zu lesen!


- üì¶ **Beliebte PocketBase-Datenbank**  
  Die Datenbank und das Interface nutzen PocketBase. Zus√§tzlich zur Webschnittstelle sind APIs f√ºr Go/JavaScript/Python verf√ºgbar.
    
    - Go: https://pocketbase.io/docs/go-overview/
    - JavaScript: https://pocketbase.io/docs/js-overview/
    - Python: https://github.com/vaphes/pocketbase

## üîÑ Unterschiede und Zusammenh√§nge zwischen Wiseflow und allgemeinen Crawler-Tools und RAG-Projekten

| Merkmal                | WiseFlow                                           | Crawler / Scraper                        | RAG-Projekte               |
|------------------------|----------------------------------------------------|------------------------------------------|----------------------------|
| **Hauptproblem gel√∂st** | Datenverarbeitung (Filterung, Extraktion, Tagging) | Rohdaten-Erfassung                       | Downstream-Anwendungen     |
| **Zusammenhang**       |                                                    | Kann in Wiseflow integriert werden, um leistungsf√§higere Rohdaten-Erfassung zu erm√∂glichen | Kann Wiseflow als dynamische Wissensdatenbank integrieren |

## üì• Installation und Verwendung

WiseFlow hat fast keine Hardwareanforderungen, minimale Systemlast und ben√∂tigt keine dedizierte GPU oder CUDA (bei Verwendung von Online-LLM-Diensten).

1. **Code-Repository klonen**

     üòÑ Liken und Forken ist eine gute Angewohnheit

    ```bash
    git clone https://github.com/TeamWiseFlow/wiseflow.git
    cd wiseflow
    ```


2. **Konfiguration**

    Kopiere `env_sample` im Verzeichnis und benenne es in `.env` um, und f√ºlle deine Konfigurationsinformationen (wie LLM-Service-Tokens) wie folgt aus:

   - LLM_API_KEY # API-Schl√ºssel f√ºr den Large-Model-Inference-Service (falls du den OpenAI-Dienst nutzt, kannst du diesen Eintrag l√∂schen)
   - LLM_API_BASE # URL-Basis f√ºr den Modellservice, der OpenAI-kompatibel ist (falls du den OpenAI-Dienst nutzt, kannst du diesen Eintrag l√∂schen)
   - WS_LOG="verbose"  # Debug-Logging aktivieren, wenn nicht ben√∂tigt, l√∂schen
   - GET_INFO_MODEL # Modell f√ºr Informations-Extraktions- und Tagging-Aufgaben, standardm√§√üig gpt-3.5-turbo
   - REWRITE_MODEL # Modell f√ºr Aufgaben der Konsolidierung und Umschreibung von nahegelegenen Informationen, standardm√§√üig gpt-3.5-turbo
   - HTML_PARSE_MODEL # Modell f√ºr Web-Parsing (intelligent aktiviert, wenn der GNE-Algorithmus unzureichend ist), standardm√§√üig gpt-3.5-turbo
   - PROJECT_DIR # Speicherort f√ºr Cache- und Log-Dateien, relativ zum Code-Repository; standardm√§√üig das Code-Repository selbst, wenn nicht angegeben
   - PB_API_AUTH='email|password' # Admin-E-Mail und Passwort f√ºr die pb-Datenbank (verwende eine g√ºltige E-Mail-Adresse f√ºr die erste Verwendung, sie kann fiktiv sein, muss aber eine E-Mail-Adresse sein)
   - PB_API_BASE  # Nicht erforderlich f√ºr den normalen Gebrauch, nur notwendig, wenn du nicht die standardm√§√üige PocketBase-Local-Interface (Port 8090) verwendest.


3. **Modell-Empfehlung**

    Nach wiederholten Tests (auf chinesischen und englischen Aufgaben) empfehlen wir f√ºr **GET_INFO_MODEL**, **REWRITE_MODEL**, und **HTML_PARSE_MODEL** die folgenden Modelle f√ºr optimale Gesamteffekt und Kosten: **"zhipuai/glm4-9B-chat"**, **"alibaba/Qwen2-7B-Instruct"**, **"alibaba/Qwen2-7B-Instruct"**.

    Diese Modelle passen gut zum Projekt, sind in der Befolgung von Anweisungen stabil und haben hervorragende Generierungseffekte. Die zugeh√∂rigen Prompts f√ºr dieses Projekt sind ebenfalls f√ºr diese drei Modelle optimiert. (**HTML_PARSE_MODEL** kann auch **"01-ai/Yi-1.5-9B-Chat"** verwenden, das in den Tests ebenfalls sehr gut abgeschnitten hat)

‚ö†Ô∏è Wir empfehlen dringend, den **SiliconFlow** Online-Inference-Service f√ºr niedrigere Kosten, schnellere Geschwindigkeiten und h√∂here kostenlose Quoten zu verwenden! ‚ö†Ô∏è

Der SiliconFlow Online-Inference-Service ist mit dem OpenAI SDK kompatibel und bietet Open-Service f√ºr die oben genannten drei Modelle. Konfiguriere LLM_API_BASE als "https://api.siliconflow.cn/v1" und LLM_API_KEY, um es zu verwenden.


4. **Lokale Bereitstellung**

    Wie du sehen kannst, verwendet dieses Projekt 7B/9B-LLMs und ben√∂tigt keine Vektormodelle, was bedeutet, dass du dieses Projekt vollst√§ndig lokal mit nur einer RTX 3090 (24 GB VRAM) bereitstellen kannst.

    Stelle sicher, dass dein lokaler LLM-Dienst mit dem OpenAI SDK kompatibel ist und konfiguriere LLM_API_BASE entsprechend.


5. **Programm ausf√ºhren**

    **F√ºr regul√§re Benutzer wird dringend empfohlen, Docker zu verwenden, um Chief Intelligence Officer auszuf√ºhren.**

    üìö F√ºr Entwickler siehe [/core/README.md](/core/README.md) f√ºr weitere Informationen.

    Zugriff auf die erfassten Daten √ºber PocketBase:

    - http://127.0.0.1:8090/_/ - Admin-Dashboard-Interface
    - http://127.0.0.1:8090/api/ - REST-API
    - https://pocketbase.io/docs/ f√ºr mehr Informationen


6. **Geplanten Quellen-Scan hinzuf√ºgen**

    Nachdem das Programm gestartet wurde, √∂ffne das Admin-Dashboard-Interface von PocketBase (http://127.0.0.1:8090/_/)

    √ñffne das Formular **sites**.

    √úber dieses Formular kannst du benutzerdefinierte Quellen angeben, und das System wird Hintergrundaufgaben starten, um die Quellen lokal zu scannen, zu parsen und zu analysieren.

    Felderbeschreibung des Formulars sites:

   - url: Die URL der Quelle. Die Quelle muss nicht die spezifische Artikelseite angeben, nur die Artikelliste-Seite. Der Wiseflow-Client enth√§lt zwei allgemeine Seitenparser, die effizient mehr als 90% der statischen Nachrichtenwebseiten erfassen und parsen k√∂nnen.
   - per_hours: H√§ufigkeit des Scannens, in Stunden, ganzzahlig (Bereich 1~24; wir empfehlen eine Scanfrequenz von einmal pro Tag, also auf 24 eingestellt).
   - activated: Ob aktiviert. Wenn deaktiviert, wird die Quelle ignoriert; sie kann sp√§ter wieder aktiviert werden.

## üõ°Ô∏è Lizenz

Dieses Projekt ist unter der [Apache 2.0](LICENSE) Lizenz als Open-Source verf√ºgbar.

F√ºr kommerzielle Nutzung und ma√ügeschneiderte Kooperationen kontaktieren Sie uns bitte unter **E-Mail: 35252986@qq.com**.

- Kommerzielle Kunden, bitte registrieren Sie sich bei uns. Das Produkt verspricht f√ºr immer kostenlos zu sein.
- F√ºr ma√ügeschneiderte Kunden bieten wir folgende Dienstleistungen basierend auf Ihren Quellen und gesch√§ftlichen Anforderungen:
  - Dedizierter Crawler und Parser f√ºr Kunden-Gesch√§ftsszenario-Quellen
  - Angepasste Strategien zur Informationsextraktion und -klassifizierung
  - Zielgerichtete LLM-Empfehlungen oder sogar Feinabstimmungsdienste
  - Dienstleistungen f√ºr private Bereitstellungen
  - Anpassung der Benutzeroberfl√§che

## üì¨ Kontaktinformationen

Wenn Sie Fragen oder Anregungen haben, k√∂nnen Sie uns gerne √ºber [Issue](https://github.com/TeamWiseFlow/wiseflow/issues) kontaktieren.

## ü§ù Dieses Projekt basiert auf den folgenden ausgezeichneten Open-Source-Projekten:

- GeneralNewsExtractor (General Extractor of News Web Page Body Based on Statistical Learning) https://github.com/GeneralNewsExtractor/GeneralNewsExtractor
- json_repair (Reparatur ung√ºltiger JSON-Dokumente) https://github.com/josdejong/jsonrepair/tree/main 
- python-pocketbase (PocketBase Client SDK f√ºr Python) https://github.com/vaphes/pocketbase

# Zitierung

Wenn Sie Teile oder das gesamte Projekt in Ihrer Arbeit verwenden oder zitieren, geben Sie bitte die folgenden Informationen an:

```
Author: Wiseflow Team
https://openi.pcl.ac.cn/wiseflow/wiseflow
https://github.com/TeamWiseFlow/wiseflow
Licensed under Apache2.0
```
